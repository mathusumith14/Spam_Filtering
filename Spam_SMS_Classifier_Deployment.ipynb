{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK5ExwA1XHMS"
      },
      "outputs": [],
      "source": [
        "import numpy as np # scientific computation\n",
        "import pandas as pd # loading dataset file\n",
        "import matplotlib.pyplot as plt # visulizatiion\n",
        "import nltk # preprocessing our text\n",
        "from nltk.corpus import stopwords # removing all the stop words\n",
        "from nlkt.stem.porter import porterstemmer # stemming of words\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/spam.csv', 'r') as dataset:\n",
        "  df = pd.read_csv(dataset,encoding=\"latin\")\n",
        "\n",
        "df.head()\n",
        "\n",
        "#give concise summary of dataframe\n",
        "df.info()\n",
        "\n",
        "#return the sum of all no values\n",
        "df.isna().sum()\n",
        "\n",
        "df.rename({\"v1\":\"label\",\"v2\":\"text\"},inplace=true,axis=1)\n",
        "#bottom 5rows of the dataframe\n",
        "df.tail()\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le=LabelEncoder()\n",
        "df['label']=le.fit_transform(df['label']\n",
        "\n",
        "#splittting data train and validation sets using tarin_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y test_size=0,20,random_state=0)\n",
        "##train size 80%and test size20%\n",
        "print(\"before oversampling,counts of label '1':{}\".format(sum(y_train==0)))\n",
        "print(\"before oversampling, counts of label'0':{}\\n\".format(sum(y_train==0)))\n",
        "\n",
        "#impoort SMOTE module from imblearn library\n",
        "#pip install imlearn (if you dont have imblearn in your system)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm=SMOTE(random_state=2)\n",
        "x_train_res,y_train_res=sm.fit_resample(x_train,y_train.ravel())\n",
        "\n",
        "print ('after oversampling,the shape of train_x:{}\".format(x_train_res.shape))\n",
        "print('after oversampling,the shape of train_y:{}\\n'.format(y_train_res.shape))\n",
        "print(\"after oversampling,countsof label'1':{}\".format(sum(y_train_res==1)))\n",
        "print(\"after oversampling,counts of label'0':{}\".format(sum(y_train_res==0)))\n",
        "\n",
        "nlkt.download(\"stopwords\")\n",
        "\n",
        "import nlkt\n",
        "from nlkt.corpus import stopwords\n",
        "from nlkt.stem import porterstemmer\n",
        "\n",
        "import re\n",
        "corpus=[]\n",
        "length=len(df)\n",
        "\n",
        "for i in range(0,length):\n",
        "    text=re.sub(\"[^a-ZA-Z0-9]\",\"\",df[\"text\"][i])\n",
        "    text=text.lower()\n",
        "    pe=porterStemmer()\n",
        "    stopword=stopwords.words(\"english\")\n",
        "    text=[pe.stem(word)for world in text if not word in set(stopword)]\n",
        "    text=\" \".join(text)\n",
        "    corpus.append(text)\n",
        "\n",
        "from skearn.feature_extraction.text import countVectorizer\n",
        "cv=countVectorizer(max_features=35000)\n",
        "x=cv.fit_transform(corpus).toarray()\n",
        "\n",
        "import pickle ##  importing pickle used for dumping models\n",
        "pickle.dump(cv,open('cv1.pkl', 'wb'))## saving to into cv.pkl file \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ifygsldzY_h1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}